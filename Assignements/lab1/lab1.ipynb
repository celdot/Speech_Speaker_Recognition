{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['samples', 'samplingrate', 'frames', 'preemph', 'windowed', 'spec', 'mspec', 'mfcc', 'lmfcc'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = np.load('lab1_example.npz', allow_pickle=True)['example'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **samples**: speech samples for one utterance\n",
    "* **samplingrate**: sampling rate\n",
    "* **frames**: speech samples organized in overlapping frames\n",
    "* **preemph**: pre-emphasized speech samples\n",
    "* **windowed**: hamming windowed speech samples\n",
    "* **spec**: squared absolute value of Fast Fourier Transform\n",
    "* **mspec**: natural log of spec multiplied by Mel filterbank\n",
    "* **mfcc**: Mel Frequency Cepstrum Coefficients\n",
    "* **lmfcc**: Liftered Mel Frequency Cepstrum Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files contain the spoken utterances of isolated digits (2 instances each).\n",
    "The digits are \"oh\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('lab1_data.npz', allow_pickle=True)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **filename**: filename of the wave file in the database\n",
    "* **samplingrate**: sampling rate of the speech signal (20kHz in all examples)\n",
    "* **gender**: gender of the speaker for the current utterance (man, woman)\n",
    "* **speaker**: speaker ID for the current utterance (ae, ac)\n",
    "* **digit**: digit contained in the current utterance (o, z, 1, ..., 9)\n",
    "* **repetition**: whether this was the first (a) or second (b) repetition\n",
    "* **samples**: array of speech samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mel Frequency Cepstrum Coefficients step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Enframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `enframe` function in lab1_proto.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take as input speech samples, the frame length in samples and the number of samples overlap between consecutive frames and outputs a two dimensional array where each row is a frame of samples. Consider only the frames that fit into the original signal disregarding extra samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the enframe function to the utterance `example['samples']` with window length of 20 milliseconds and shift of 10 ms (figure out the length and shift in samples from the sampling rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pcolormesh function from matplotlib.pyplot to plot the resulting array. Verify that your result corresponds to the array in example['frames']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Pre-emphasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `preemp` function in `lab1_proto.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, define a pre-emphasis filter with pre-emphasis coefficient 0.97 using the `lfilter` function from `scipy.signal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how you defined the filter coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the filter to each frame in the output from the `enframe` function. This should correspond to the `example['preemph']` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hamming Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `windowing` function in `lab1_proto.py.` \n",
    "\n",
    "To do this, define a hamming window of the correct size using the `hamming` function from `scipy.signal` with extra option `sym=False`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the window shape and explain why this windowing should be applied to the frames of speech signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply hamming window to the pre-emphasized frames of the previous step. This should correspond to the `example['windowed']` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `powerSpectrum` function in `lab1_proto.py`. To do this, compute the Fast Fourier Transform (FFT) of the input from `scipy.fftpack` and then the squared modulus of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Apply your function to the windowed speech frames, with FFT length of 512 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting power spectrogram with `pcolormesh`. Beware of the fact that the FFT bins correspond to frequencies that go from 0 to fmax and back to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is fmax in this case according to the Sampling Theorem? The array should correspond to example['spec']."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Mel filterbank log spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `logMelSpectrum` function in `lab1_proto.py`. Use the `trfbank` function, provided\n",
    "in the `lab1_tools.py` file, to create a bank of triangular filters linearly spaced in the Mel\n",
    "frequency scale. Plot the filters in linear frequency scale. Describe the distribution of the\n",
    "f\n",
    "ilters along the frequency axis. Apply the filters to the output of the power spectrum from the\n",
    "previous step for each frame and take the natural log of the result. Plot the resulting filterbank\n",
    "outputs with `pcolormesh`. This should correspond to the `example['mspec']` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Cosine Transofrm and Liftering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `cepstrum` function in `lab1_proto.py`. To do this, apply the Discrete Cosine Transform (`dct` function from `scipy.fftpack.realtransforms`) to the outputs of the filterbank.\n",
    "\n",
    "Use coefficients from 0 to 12 (13 coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that using the $n=13$ input parameter in `dct` is not the same as running without the argument and taking the first 13 elements in the results.\n",
    "\n",
    "Note that $n=13$ does not only determine the number of output DCT coefficients the function returns, but will also truncate the input in case n is smaller than the input length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply liftering using the function `lifter` in `lab1_tools.py`. This last step is used to correct the range of the coefficients. Plot the resulting coefficients with `pcolormesh`. These should correspond to `example['mfcc']` and `example['lmfcc']` respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are sure all the above steps are correct, use the `mfcc` function (`lab1_proto.py`) to compute the liftered MFCCs for all the utterances in the `data` array.\n",
    "\n",
    "Observe differences for different utterances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate all the MFCC frames from all utterances in the data array into a big feature\n",
    "[N ×M] array where N is the total number of frames in the data set and M is the number of\n",
    "coefficients. Then compute the correlation coefficients between features4 and display the result\n",
    "with pcolormesh. Are features correlated? Is the assumption of diagonal covariance matrices\n",
    "for Gaussian modelling justified? Compare the results you obtain for the MFCC features with\n",
    "those obtained with the Mel filterbank features ('mspec' features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Speech Segments with Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the concatenated data from the previous section, train a Gaussian mixture model with sklearn.mixture.GMM (or sklearn.mixture.GaussianMixture depending on the version of sklearn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the number of components for example: 4, 8, 16, 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are trained, compute GMM posteriors for utterances containing the same words.\n",
    "Plot the results with pcolormesh and observe the evolution of the GMM posteriors in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Can you say something about the classes discovered by the unsupervised learning method?\n",
    "* Do the classes roughly correspond to the phonemes you expect to compose each word?\n",
    "* Are those classes a stable representation of the word if you compare utterances from different speakers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, plot and discuss the GMM posteriors for the model with 32 components for the four occurrences of the word “seven” (utterances 16, 17, 38, and 39)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two utterances of length $N$ and $M$ respectively, compute an $[N × M]$ matrix of local Euclidean distances between each MFCC vector in the first utterance and each MFCC vector in the second utterance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called dtw (lab1_proto.py) that takes as input this matrix of local distances between MFCC vector in the first utterance and each MFCC vector in the second utterance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called dtw (lab1_proto.py) that takes as input this matrix of local distances and outputs the result of the Dynamic Time Warping algorithm. The main output is the global distance between the two sequences (utterances), but you may want to output also the best path for debugging reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of utterances in the data array:\n",
    "\n",
    "1. compute the local Euclidean distances between MFCC vectors in the first and second utterance\n",
    "2. compute the global distance between utterances with the dtw function you have written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the global pairwise distances in a matrix D (44×44).\n",
    "Display the matrix with pcolormesh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare distances within the same digit and across different digits.\n",
    "Does the distance separate digits well even between different speakers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run hierarchical clustering on the distance matrix D using the linkage function from scipy.cluster.hierarchy.\n",
    "Use the ”complete” linkage method.\n",
    "Display the results with the function dendrogram from the same library, and comment them.\n",
    "Use the tidigit2labels function (lab1_tools.py) to create labels to add to the dendrogram to simplify the interpretation of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
